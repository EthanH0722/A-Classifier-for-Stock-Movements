{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFile2018 = 'C:\\\\Users\\\\hycwy\\\\Desktop\\\\Python data\\\\Stock 200 indicator\\\\2018_Financial_Data.csv'\n",
    "trainFile2017 = 'C:\\\\Users\\\\hycwy\\\\Desktop\\\\Python data\\\\Stock 200 indicator\\\\2017_Financial_Data.csv'\n",
    "\n",
    "pwd = os.getcwd()\n",
    "os.chdir(os.path.dirname(trainFile2018))\n",
    "trainData1 = pd.read_csv(os.path.basename(trainFile2018))\n",
    "trainData2 = pd.read_csv(os.path.basename(trainFile2017))\n",
    "os.chdir(pwd)\n",
    "\n",
    "trainData = pd.concat([trainData1,trainData2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = trainData1.iloc[0:4392,0:225] #when use larger dataset[0:9353,0:224], model performs worse\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "le = LabelEncoder().fit_transform(data['Sector'])\n",
    "ohe = OneHotEncoder(sparse=False).fit_transform(le.reshape(-1,1))\n",
    "enc = ohe.astype(int)\n",
    "sector_new = pd.DataFrame(enc) \n",
    "\n",
    "data_class = data['Class']\n",
    "data_drop = data.drop(['Sector','Class'],axis=1)\n",
    "\n",
    "data = pd.concat([data_drop,sector_new],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:235]\n",
    "y = data_class\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "X_test_scaled = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set: 0.996357\n",
      "accuracy on test set: 0.925319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hycwy\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lclf = LinearSVC().fit(X_train_scaled,y_train)\n",
    "print('accuracy on training set: %f' % lclf.score(X_train_scaled,y_train))\n",
    "print('accuracy on test set: %f' % lclf.score(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set: 0.999696\n",
      "accuracy on test set: 0.903461\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=100, kernel='rbf', gamma=0.01).fit(X_train_scaled,y_train)\n",
    "\n",
    "print('accuracy on training set: %f' % clf.score(X_train_scaled,y_train))\n",
    "print('accuracy on test set: %f' % clf.score(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_func(C,gamma):\n",
    "    svcclf = SVC(C=C,gamma=gamma,kernel='rbf').fit(X_train_scaled,y_train)\n",
    "    return svcclf.score(X_test_scaled,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     C     |   gamma   |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8124  \u001b[0m | \u001b[0m 55.33   \u001b[0m | \u001b[0m 0.07437 \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8306  \u001b[0m | \u001b[95m 60.67   \u001b[0m | \u001b[95m 0.05904 \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.8133  \u001b[0m | \u001b[0m 42.94   \u001b[0m | \u001b[0m 0.06813 \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7878  \u001b[0m | \u001b[0m 44.32   \u001b[0m | \u001b[0m 0.09026 \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.8552  \u001b[0m | \u001b[95m 96.4    \u001b[0m | \u001b[95m 0.04451 \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7823  \u001b[0m | \u001b[0m 95.33   \u001b[0m | \u001b[0m 0.1     \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.8634  \u001b[0m | \u001b[95m 96.4    \u001b[0m | \u001b[95m 0.04079 \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8151  \u001b[0m | \u001b[0m 83.32   \u001b[0m | \u001b[0m 0.06759 \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8479  \u001b[0m | \u001b[0m 96.37   \u001b[0m | \u001b[0m 0.04798 \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.9035  \u001b[0m | \u001b[95m 96.42   \u001b[0m | \u001b[95m 0.01403 \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9007  \u001b[0m | \u001b[0m 96.44   \u001b[0m | \u001b[0m 0.01202 \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8552  \u001b[0m | \u001b[0m 96.41   \u001b[0m | \u001b[0m 0.04475 \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7942  \u001b[0m | \u001b[0m 9.078   \u001b[0m | \u001b[0m 0.08329 \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8843  \u001b[0m | \u001b[0m 96.52   \u001b[0m | \u001b[0m 0.02277 \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8862  \u001b[0m | \u001b[0m 83.23   \u001b[0m | \u001b[0m 0.02514 \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "pbounds = {'C':(1,100),'gamma':(0.01,0.1)}\n",
    "optimizersvc = BayesianOptimization(\n",
    "    f=svc_func,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,\n",
    "    random_state=0,\n",
    ")\n",
    "optimizersvc.maximize(n_iter=10,init_points=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set: 0.470249\n",
      "accuracy on test set: 0.476321\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(solver='adam',random_state=0,hidden_layer_sizes=[100,100],max_iter=300,activation='logistic',learning_rate_init=0.1,learning_rate='adaptive',alpha=0.01).fit(X_train,y_train)\n",
    "\n",
    "print('accuracy on training set: %f' % mlp.score(X_train_scaled,y_train))\n",
    "print('accuracy on test set: %f' % mlp.score(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def ml_func1(\n",
    "            hidden_layer_sizes1,\n",
    "            max_iter,\n",
    "            learning_rate_init,\n",
    "            alpha):\n",
    "    \n",
    "    layer1 = int(hidden_layer_sizes1)\n",
    "    \n",
    "    ml = MLPClassifier(solver='adam',\n",
    "                       random_state=0,\n",
    "                       hidden_layer_sizes=[layer1],\n",
    "                       max_iter=int(max_iter),\n",
    "                       activation='logistic',\n",
    "                       learning_rate_init=learning_rate_init,\n",
    "                       learning_rate='adaptive',\n",
    "                       alpha=alpha).fit(X_train,y_train)\n",
    "    return ml.score(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "pbounds = {'max_iter':(100,600),'learning_rate_init':(0.01,1),'alpha':(0.01,1),'hidden_layer_sizes1':(100,300)}\n",
    "optimizer1 = BayesianOptimization(\n",
    "    f=ml_func1,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   | hidden... | learni... | max_iter  |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7092  \u001b[0m | \u001b[0m 0.4229  \u001b[0m | \u001b[0m 244.1   \u001b[0m | \u001b[0m 0.01011 \u001b[0m | \u001b[0m 251.2   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.3254  \u001b[0m | \u001b[0m 0.1553  \u001b[0m | \u001b[0m 118.5   \u001b[0m | \u001b[0m 0.1944  \u001b[0m | \u001b[0m 272.8   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6627  \u001b[0m | \u001b[0m 0.4028  \u001b[0m | \u001b[0m 207.8   \u001b[0m | \u001b[0m 0.425   \u001b[0m | \u001b[0m 442.6   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7043  \u001b[0m | \u001b[0m 0.2124  \u001b[0m | \u001b[0m 275.6   \u001b[0m | \u001b[0m 0.03711 \u001b[0m | \u001b[0m 435.2   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6885  \u001b[0m | \u001b[0m 0.4231  \u001b[0m | \u001b[0m 211.7   \u001b[0m | \u001b[0m 0.149   \u001b[0m | \u001b[0m 199.1   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6946  \u001b[0m | \u001b[0m 0.9182  \u001b[0m | \u001b[0m 242.3   \u001b[0m | \u001b[0m 0.5583  \u001b[0m | \u001b[0m 252.3   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7086  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 271.5   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 217.0   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.7137  \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 290.3   \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 265.1   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5932  \u001b[0m | \u001b[0m 0.05791 \u001b[0m | \u001b[0m 295.6   \u001b[0m | \u001b[0m 0.1394  \u001b[0m | \u001b[0m 371.8   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.1975  \u001b[0m | \u001b[0m 272.2   \u001b[0m | \u001b[0m 0.4901  \u001b[0m | \u001b[0m 492.3   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6463  \u001b[0m | \u001b[0m 0.02716 \u001b[0m | \u001b[0m 243.2   \u001b[0m | \u001b[0m 0.7264  \u001b[0m | \u001b[0m 141.5   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7028  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 216.1   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 520.2   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.374   \u001b[0m | \u001b[0m 0.3831  \u001b[0m | \u001b[0m 259.7   \u001b[0m | \u001b[0m 0.4556  \u001b[0m | \u001b[0m 554.2   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.3239  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 179.2   \u001b[0m | \u001b[0m 0.2279  \u001b[0m | \u001b[0m 497.8   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7043  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 240.6   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 220.0   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.6919  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 248.1   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 462.2   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.6937  \u001b[0m | \u001b[0m 0.7952  \u001b[0m | \u001b[0m 299.5   \u001b[0m | \u001b[0m 0.2091  \u001b[0m | \u001b[0m 464.0   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.7019  \u001b[0m | \u001b[0m 0.01306 \u001b[0m | \u001b[0m 236.9   \u001b[0m | \u001b[0m 0.8486  \u001b[0m | \u001b[0m 406.1   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.69    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 192.3   \u001b[0m | \u001b[0m 0.373   \u001b[0m | \u001b[0m 387.4   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.7019  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 225.4   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 352.6   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.6919  \u001b[0m | \u001b[0m 0.4673  \u001b[0m | \u001b[0m 268.3   \u001b[0m | \u001b[0m 0.2944  \u001b[0m | \u001b[0m 304.9   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.6919  \u001b[0m | \u001b[0m 0.8395  \u001b[0m | \u001b[0m 174.3   \u001b[0m | \u001b[0m 0.1478  \u001b[0m | \u001b[0m 158.0   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.6785  \u001b[0m | \u001b[0m 0.1872  \u001b[0m | \u001b[0m 167.7   \u001b[0m | \u001b[0m 0.7851  \u001b[0m | \u001b[0m 106.1   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.3376  \u001b[0m | \u001b[0m 0.2736  \u001b[0m | \u001b[0m 125.8   \u001b[0m | \u001b[0m 0.6813  \u001b[0m | \u001b[0m 140.9   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.3245  \u001b[0m | \u001b[0m 0.6699  \u001b[0m | \u001b[0m 208.5   \u001b[0m | \u001b[0m 0.7205  \u001b[0m | \u001b[0m 102.6   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.8321  \u001b[0m | \u001b[0m 272.8   \u001b[0m | \u001b[0m 0.9582  \u001b[0m | \u001b[0m 244.5   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.4119  \u001b[0m | \u001b[0m 253.8   \u001b[0m | \u001b[0m 0.8168  \u001b[0m | \u001b[0m 182.2   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.8363  \u001b[0m | \u001b[0m 185.8   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 343.9   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.7086  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 220.5   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 313.3   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.7013  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 300.0   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 180.7   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.3352  \u001b[0m | \u001b[0m 0.2985  \u001b[0m | \u001b[0m 295.4   \u001b[0m | \u001b[0m 0.5922  \u001b[0m | \u001b[0m 138.5   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.7049  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 300.0   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 212.7   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.663   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 147.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 376.3   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.5853  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 171.3   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 199.2   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.3594  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 211.9   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 165.1   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.7025  \u001b[0m | \u001b[0m 0.3563  \u001b[0m | \u001b[0m 260.1   \u001b[0m | \u001b[0m 0.081   \u001b[0m | \u001b[0m 276.7   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.6655  \u001b[0m | \u001b[0m 0.6404  \u001b[0m | \u001b[0m 248.0   \u001b[0m | \u001b[0m 0.6274  \u001b[0m | \u001b[0m 332.1   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.6991  \u001b[0m | \u001b[0m 0.9984  \u001b[0m | \u001b[0m 237.2   \u001b[0m | \u001b[0m 0.9638  \u001b[0m | \u001b[0m 496.6   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.3097  \u001b[0m | \u001b[0m 0.7967  \u001b[0m | \u001b[0m 299.4   \u001b[0m | \u001b[0m 0.7442  \u001b[0m | \u001b[0m 299.5   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.5847  \u001b[0m | \u001b[0m 0.9496  \u001b[0m | \u001b[0m 241.2   \u001b[0m | \u001b[0m 0.4734  \u001b[0m | \u001b[0m 297.3   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.3629  \u001b[0m | \u001b[0m 279.4   \u001b[0m | \u001b[0m 0.2899  \u001b[0m | \u001b[0m 193.8   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.6964  \u001b[0m | \u001b[0m 0.6284  \u001b[0m | \u001b[0m 246.0   \u001b[0m | \u001b[0m 0.133   \u001b[0m | \u001b[0m 431.9   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.6913  \u001b[0m | \u001b[0m 0.9227  \u001b[0m | \u001b[0m 192.6   \u001b[0m | \u001b[0m 0.4185  \u001b[0m | \u001b[0m 313.6   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.2324  \u001b[0m | \u001b[0m 220.1   \u001b[0m | \u001b[0m 0.893   \u001b[0m | \u001b[0m 380.8   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.704   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 207.6   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 228.2   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.697   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 267.7   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 404.1   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.7077  \u001b[0m | \u001b[0m 0.6799  \u001b[0m | \u001b[0m 166.9   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 415.9   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.3103  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 131.7   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 417.8   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.3852  \u001b[0m | \u001b[0m 0.6543  \u001b[0m | \u001b[0m 196.4   \u001b[0m | \u001b[0m 0.6166  \u001b[0m | \u001b[0m 416.6   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.691   \u001b[0m | \u001b[0m 0.7735  \u001b[0m | \u001b[0m 212.0   \u001b[0m | \u001b[0m 0.3407  \u001b[0m | \u001b[0m 334.5   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.01449 \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 0.8402  \u001b[0m | \u001b[0m 365.1   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.6949  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 249.4   \u001b[0m | \u001b[0m 0.8528  \u001b[0m | \u001b[0m 378.0   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.7049  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 300.0   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 241.5   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.6913  \u001b[0m | \u001b[0m 0.7099  \u001b[0m | \u001b[0m 172.3   \u001b[0m | \u001b[0m 0.892   \u001b[0m | \u001b[0m 372.8   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.7022  \u001b[0m | \u001b[0m 0.1405  \u001b[0m | \u001b[0m 272.8   \u001b[0m | \u001b[0m 0.04034 \u001b[0m | \u001b[0m 461.7   \u001b[0m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer1.maximize(n_iter=50,init_points=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ml_func2(\n",
    "            hidden_layer_sizes1,\n",
    "            hidden_layer_sizes2,\n",
    "            max_iter,\n",
    "            learning_rate_init,\n",
    "            alpha):\n",
    "    \n",
    "    layer1 = int(hidden_layer_sizes1)\n",
    "    layer2 = int(hidden_layer_sizes2)\n",
    "    ml = MLPClassifier(solver='adam',\n",
    "                       random_state=0,\n",
    "                       hidden_layer_sizes=[layer1,layer2],\n",
    "                       max_iter=int(max_iter),\n",
    "                       activation='logistic',\n",
    "                       learning_rate_init=learning_rate_init,\n",
    "                       learning_rate='adaptive',\n",
    "                       alpha=alpha).fit(X_train,y_train)\n",
    "    return ml.score(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "pbounds = {'max_iter':(100,600),'learning_rate_init':(0.01,1),'alpha':(0.01,1),'hidden_layer_sizes1':(100,300),'hidden_layer_sizes2':(200,400)}\n",
    "optimizer2 = BayesianOptimization(\n",
    "    f=ml_func2,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   | hidden... | hidden... | learni... | max_iter  |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.8957  \u001b[0m | \u001b[0m 117.0   \u001b[0m | \u001b[0m 207.8   \u001b[0m | \u001b[0m 0.1781  \u001b[0m | \u001b[0m 539.1   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 0.1074  \u001b[0m | \u001b[0m 184.2   \u001b[0m | \u001b[0m 391.6   \u001b[0m | \u001b[0m 0.5378  \u001b[0m | \u001b[0m 445.9   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.7031  \u001b[0m | \u001b[95m 0.3224  \u001b[0m | \u001b[95m 237.3   \u001b[0m | \u001b[95m 366.9   \u001b[0m | \u001b[95m 0.02811 \u001b[0m | \u001b[95m 475.1   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.989   \u001b[0m | \u001b[0m 249.6   \u001b[0m | \u001b[0m 256.1   \u001b[0m | \u001b[0m 0.7914  \u001b[0m | \u001b[0m 151.6   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.4534  \u001b[0m | \u001b[0m 281.7   \u001b[0m | \u001b[0m 258.7   \u001b[0m | \u001b[0m 0.2949  \u001b[0m | \u001b[0m 165.0   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.4889  \u001b[0m | \u001b[0m 251.5   \u001b[0m | \u001b[0m 254.9   \u001b[0m | \u001b[0m 0.5075  \u001b[0m | \u001b[0m 151.7   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6627  \u001b[0m | \u001b[0m 0.5169  \u001b[0m | \u001b[0m 285.9   \u001b[0m | \u001b[0m 345.3   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 501.1   \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m 0.7046  \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 229.6   \u001b[0m | \u001b[95m 252.0   \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 239.2   \u001b[0m |\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 0.7064  \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 166.9   \u001b[0m | \u001b[95m 229.1   \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 198.7   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.9384  \u001b[0m | \u001b[0m 161.2   \u001b[0m | \u001b[0m 256.4   \u001b[0m | \u001b[0m 0.04988 \u001b[0m | \u001b[0m 273.2   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.03791 \u001b[0m | \u001b[0m 149.4   \u001b[0m | \u001b[0m 266.8   \u001b[0m | \u001b[0m 0.885   \u001b[0m | \u001b[0m 129.7   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 270.6   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 204.4   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.8828  \u001b[0m | \u001b[0m 271.3   \u001b[0m | \u001b[0m 316.8   \u001b[0m | \u001b[0m 0.04417 \u001b[0m | \u001b[0m 238.8   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 238.3   \u001b[0m | \u001b[0m 268.9   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 315.6   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.7019  \u001b[0m | \u001b[0m 0.2742  \u001b[0m | \u001b[0m 110.1   \u001b[0m | \u001b[0m 204.7   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 255.3   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 279.0   \u001b[0m | \u001b[0m 0.9428  \u001b[0m | \u001b[0m 295.0   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 202.7   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 338.1   \u001b[0m |\n",
      "| \u001b[95m 23      \u001b[0m | \u001b[95m 0.7067  \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 264.6   \u001b[0m | \u001b[95m 352.0   \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 168.8   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.8185  \u001b[0m | \u001b[0m 209.4   \u001b[0m | \u001b[0m 338.0   \u001b[0m | \u001b[0m 0.2002  \u001b[0m | \u001b[0m 106.2   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.8424  \u001b[0m | \u001b[0m 290.4   \u001b[0m | \u001b[0m 319.4   \u001b[0m | \u001b[0m 0.6594  \u001b[0m | \u001b[0m 101.7   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.4166  \u001b[0m | \u001b[0m 298.8   \u001b[0m | \u001b[0m 399.1   \u001b[0m | \u001b[0m 0.7778  \u001b[0m | \u001b[0m 223.3   \u001b[0m |\n",
      "| \u001b[95m 27      \u001b[0m | \u001b[95m 0.7116  \u001b[0m | \u001b[95m 0.9481  \u001b[0m | \u001b[95m 202.7   \u001b[0m | \u001b[95m 393.3   \u001b[0m | \u001b[95m 0.4035  \u001b[0m | \u001b[95m 184.7   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.6089  \u001b[0m | \u001b[0m 130.3   \u001b[0m | \u001b[0m 372.4   \u001b[0m | \u001b[0m 0.7215  \u001b[0m | \u001b[0m 157.2   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 0.4977  \u001b[0m | \u001b[0m 144.8   \u001b[0m | \u001b[0m 395.0   \u001b[0m | \u001b[0m 0.6791  \u001b[0m | \u001b[0m 238.9   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 0.06903 \u001b[0m | \u001b[0m 222.0   \u001b[0m | \u001b[0m 397.3   \u001b[0m | \u001b[0m 0.9372  \u001b[0m | \u001b[0m 123.4   \u001b[0m |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer2.maximize(n_iter=20,init_points=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_func3(\n",
    "            hidden_layer_sizes1,\n",
    "            hidden_layer_sizes2,\n",
    "            hidden_layer_sizes3,\n",
    "            max_iter,\n",
    "            learning_rate_init,\n",
    "            alpha):\n",
    "    \n",
    "    layer1 = int(hidden_layer_sizes1)\n",
    "    layer2 = int(hidden_layer_sizes2)\n",
    "    layer3 = int(hidden_layer_sizes3)\n",
    "    ml = MLPClassifier(solver='adam',\n",
    "                       random_state=0,\n",
    "                       hidden_layer_sizes=[layer1,layer2,layer3],\n",
    "                       max_iter=int(max_iter),\n",
    "                       activation='logistic',\n",
    "                       learning_rate_init=learning_rate_init,\n",
    "                       learning_rate='adaptive',\n",
    "                       alpha=alpha).fit(X_train,y_train)\n",
    "    return ml.score(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "pbounds = {'max_iter':(100,600),'learning_rate_init':(0.01,1),'alpha':(0.01,1),'hidden_layer_sizes1':(100,300),'hidden_layer_sizes2':(200,400),'hidden_layer_sizes3':(100,300)}\n",
    "optimizer3 = BayesianOptimization(\n",
    "    f=ml_func3,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   | hidden... | hidden... | hidden... | learni... | max_iter  |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.4229  \u001b[0m | \u001b[0m 244.1   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 160.5   \u001b[0m | \u001b[0m 0.1553  \u001b[0m | \u001b[0m 146.2   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.1944  \u001b[0m | \u001b[0m 169.1   \u001b[0m | \u001b[0m 279.4   \u001b[0m | \u001b[0m 207.8   \u001b[0m | \u001b[0m 0.425   \u001b[0m | \u001b[0m 442.6   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.2124  \u001b[0m | \u001b[0m 275.6   \u001b[0m | \u001b[0m 205.5   \u001b[0m | \u001b[0m 234.1   \u001b[0m | \u001b[0m 0.4231  \u001b[0m | \u001b[0m 379.3   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.149   \u001b[0m | \u001b[0m 139.6   \u001b[0m | \u001b[0m 360.1   \u001b[0m | \u001b[0m 293.7   \u001b[0m | \u001b[0m 0.3203  \u001b[0m | \u001b[0m 446.2   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.8776  \u001b[0m | \u001b[0m 278.9   \u001b[0m | \u001b[0m 217.0   \u001b[0m | \u001b[0m 107.8   \u001b[0m | \u001b[0m 0.1781  \u001b[0m | \u001b[0m 539.1   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 0.1074  \u001b[0m | \u001b[0m 184.2   \u001b[0m | \u001b[0m 391.6   \u001b[0m | \u001b[0m 206.6   \u001b[0m | \u001b[0m 0.695   \u001b[0m | \u001b[0m 257.8   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 0.6896  \u001b[0m | \u001b[0m 266.9   \u001b[0m | \u001b[0m 203.7   \u001b[0m | \u001b[0m 250.0   \u001b[0m | \u001b[0m 0.989   \u001b[0m | \u001b[0m 474.1   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.2876  \u001b[0m | \u001b[0m 257.9   \u001b[0m | \u001b[0m 220.6   \u001b[0m | \u001b[0m 189.6   \u001b[0m | \u001b[0m 0.9095  \u001b[0m | \u001b[0m 246.8   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.2949  \u001b[0m | \u001b[0m 126.0   \u001b[0m | \u001b[0m 203.9   \u001b[0m | \u001b[0m 235.8   \u001b[0m | \u001b[0m 0.2195  \u001b[0m | \u001b[0m 232.8   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 0.4967  \u001b[0m | \u001b[0m 110.7   \u001b[0m | \u001b[0m 314.8   \u001b[0m | \u001b[0m 129.3   \u001b[0m | \u001b[0m 0.5934  \u001b[0m | \u001b[0m 449.9   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.516   \u001b[0m | \u001b[0m 134.5   \u001b[0m | \u001b[0m 210.4   \u001b[0m | \u001b[0m 236.9   \u001b[0m | \u001b[0m 0.2268  \u001b[0m | \u001b[0m 235.9   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.08284 \u001b[0m | \u001b[0m 232.4   \u001b[0m | \u001b[0m 204.7   \u001b[0m | \u001b[0m 224.9   \u001b[0m | \u001b[0m 0.1649  \u001b[0m | \u001b[0m 315.9   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.689   \u001b[0m | \u001b[0m 300.0   \u001b[0m | \u001b[0m 200.6   \u001b[0m | \u001b[0m 171.8   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 328.0   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.1853  \u001b[0m | \u001b[0m 299.6   \u001b[0m | \u001b[0m 208.6   \u001b[0m | \u001b[0m 276.9   \u001b[0m | \u001b[0m 0.7695  \u001b[0m | \u001b[0m 286.5   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.06492 \u001b[0m | \u001b[0m 250.2   \u001b[0m | \u001b[0m 201.5   \u001b[0m | \u001b[0m 271.1   \u001b[0m | \u001b[0m 0.1825  \u001b[0m | \u001b[0m 180.4   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.06341 \u001b[0m | \u001b[0m 137.6   \u001b[0m | \u001b[0m 211.3   \u001b[0m | \u001b[0m 240.0   \u001b[0m | \u001b[0m 0.07981 \u001b[0m | \u001b[0m 228.9   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.0131  \u001b[0m | \u001b[0m 279.3   \u001b[0m | \u001b[0m 201.9   \u001b[0m | \u001b[0m 241.9   \u001b[0m | \u001b[0m 0.8197  \u001b[0m | \u001b[0m 377.7   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.4647  \u001b[0m | \u001b[0m 248.3   \u001b[0m | \u001b[0m 208.3   \u001b[0m | \u001b[0m 271.9   \u001b[0m | \u001b[0m 0.01721 \u001b[0m | \u001b[0m 179.5   \u001b[0m |\n",
      "| \u001b[95m 19      \u001b[0m | \u001b[95m 0.7049  \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 106.1   \u001b[0m | \u001b[95m 242.1   \u001b[0m | \u001b[95m 300.0   \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 385.1   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.6   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 126.3   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 300.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 300.0   \u001b[0m | \u001b[0m 333.3   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 600.0   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 150.6   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 219.0   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.3109  \u001b[0m | \u001b[0m 249.7   \u001b[0m | \u001b[0m 204.9   \u001b[0m | \u001b[0m 166.3   \u001b[0m | \u001b[0m 0.3111  \u001b[0m | \u001b[0m 146.6   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 176.4   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 222.7   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.733   \u001b[0m | \u001b[0m 166.9   \u001b[0m | \u001b[0m 200.5   \u001b[0m | \u001b[0m 299.3   \u001b[0m | \u001b[0m 0.8081  \u001b[0m | \u001b[0m 298.0   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.7648  \u001b[0m | \u001b[0m 190.1   \u001b[0m | \u001b[0m 289.3   \u001b[0m | \u001b[0m 289.3   \u001b[0m | \u001b[0m 0.1134  \u001b[0m | \u001b[0m 377.4   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 300.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 118.1   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.1525  \u001b[0m | \u001b[0m 105.1   \u001b[0m | \u001b[0m 261.8   \u001b[0m | \u001b[0m 282.3   \u001b[0m | \u001b[0m 0.8545  \u001b[0m | \u001b[0m 480.2   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.3084  \u001b[0m | \u001b[0m 0.07111 \u001b[0m | \u001b[0m 284.7   \u001b[0m | \u001b[0m 202.0   \u001b[0m | \u001b[0m 245.0   \u001b[0m | \u001b[0m 0.7117  \u001b[0m | \u001b[0m 383.9   \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer3.maximize(n_iter=20,init_points=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set: 1.000000\n",
      "accuracy on test set: 0.988160\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=10,random_state=3,bootstrap=True, class_weight=None, criterion='gini',\n",
    " max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    " min_samples_leaf=1, min_samples_split=2,\n",
    " min_weight_fraction_leaf=0.0, n_jobs=1,\n",
    " oob_score=False, verbose=0, warm_start=False).fit(X_train,y_train)\n",
    "\n",
    "print('accuracy on training set: %f' %forest.score(X_train,y_train))\n",
    "print('accuracy on test set: %f' %forest.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_ = X_train.columns\n",
    "weight_ = forest.feature_importances_\n",
    "len(index_)==len(weight_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "for i in (range(233)):\n",
    "    d[index_[i]]=weight_[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2019 PRICE VAR [%]', 0.48790037143622145), ('Consolidated Income', 0.017600775788965267), ('Total non-current liabilities', 0.012396380049736214), ('Invested Capital', 0.012185698865949076), ('Total assets', 0.01142655644374993), ('Total current liabilities', 0.011240856284284154), ('Total non-current assets', 0.010867583168413039), ('priceToOperatingCashFlowsRatio', 0.007983642705560134), (4, 0.005309117242912908), ('POCF ratio', 0.005277244724984531)]\n",
      "{'2019 PRICE VAR [%]': 0.48790037143622145, 'Consolidated Income': 0.017600775788965267, 'Total non-current liabilities': 0.012396380049736214, 'Invested Capital': 0.012185698865949076, 'Total assets': 0.01142655644374993, 'Total current liabilities': 0.011240856284284154, 'Total non-current assets': 0.010867583168413039, 'priceToOperatingCashFlowsRatio': 0.007983642705560134, 4: 0.005309117242912908, 'POCF ratio': 0.005277244724984531}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = 10\n",
    " \n",
    "L = sorted(d.items(),key=lambda item:item[1],reverse=True)\n",
    " \n",
    "L = L[:n]\n",
    " \n",
    "print(L)\n",
    " \n",
    "dictdata = {}\n",
    "for l in L:\n",
    "    dictdata[l[0]] = l[1]\n",
    " \n",
    "print (dictdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indicators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019 PRICE VAR [%]',\n",
       " 'Consolidated Income',\n",
       " 'Total non-current liabilities',\n",
       " 'Invested Capital',\n",
       " 'Total assets',\n",
       " 'Total current liabilities',\n",
       " 'Total non-current assets',\n",
       " 'priceToOperatingCashFlowsRatio',\n",
       " 4,\n",
       " 'POCF ratio']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in dictdata.keys():\n",
    "    new_indicators.append(i)\n",
    "\n",
    "new_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X[new_indicators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new,y,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_new = StandardScaler().fit_transform(X_train_new)\n",
    "X_test_scaled_new = StandardScaler().fit_transform(X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with Bayesian_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set: 0.984821\n",
      "accuracy on test set: 0.976321\n"
     ]
    }
   ],
   "source": [
    "clf_new = SVC(C=100, kernel='rbf', gamma=0.001).fit(X_train_scaled_new,y_train_new)\n",
    "\n",
    "print('accuracy on training set: %f' % clf_new.score(X_train_scaled_new,y_train_new))\n",
    "print('accuracy on test set: %f' % clf_new.score(X_test_scaled_new,y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC_func_new(C,gamma):\n",
    "    clf_new1 = SVC(C=C,kernel='rbf',gamma=gamma).fit(X_train_scaled_new,y_train_new)\n",
    "    return clf_new1.score(X_test_scaled_new,y_test_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     C     |   gamma   |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 548.8   \u001b[0m | \u001b[0m 715.2   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.6876  \u001b[0m | \u001b[95m 602.8   \u001b[0m | \u001b[95m 544.9   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6876  \u001b[0m | \u001b[0m 423.7   \u001b[0m | \u001b[0m 645.9   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 437.6   \u001b[0m | \u001b[0m 891.8   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.7386  \u001b[0m | \u001b[95m 963.7   \u001b[0m | \u001b[95m 383.4   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6903  \u001b[0m | \u001b[0m 791.7   \u001b[0m | \u001b[0m 528.9   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 568.0   \u001b[0m | \u001b[0m 925.6   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.8689  \u001b[0m | \u001b[95m 71.04   \u001b[0m | \u001b[95m 87.13   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 20.22   \u001b[0m | \u001b[0m 832.6   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 778.2   \u001b[0m | \u001b[0m 870.0   \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.9718  \u001b[0m | \u001b[95m 271.5   \u001b[0m | \u001b[95m 0.001   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8689  \u001b[0m | \u001b[0m 74.27   \u001b[0m | \u001b[0m 85.87   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9663  \u001b[0m | \u001b[0m 518.4   \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9117  \u001b[0m | \u001b[0m 383.3   \u001b[0m | \u001b[0m 7.016   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9654  \u001b[0m | \u001b[0m 728.9   \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.8525  \u001b[0m | \u001b[0m 623.7   \u001b[0m | \u001b[0m 136.4   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.9663  \u001b[0m | \u001b[0m 1e+03   \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 375.3   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.9663  \u001b[0m | \u001b[0m 876.6   \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.9408  \u001b[0m | \u001b[0m 14.41   \u001b[0m | \u001b[0m 2.19    \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.8561  \u001b[0m | \u001b[0m 1e+03   \u001b[0m | \u001b[0m 112.9   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.9545  \u001b[0m | \u001b[0m 615.5   \u001b[0m | \u001b[0m 0.708   \u001b[0m |\n",
      "| \u001b[95m 23      \u001b[0m | \u001b[95m 0.9745  \u001b[0m | \u001b[95m 186.8   \u001b[0m | \u001b[95m 0.001   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 1e+03   \u001b[0m | \u001b[0m 1e+03   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.9654  \u001b[0m | \u001b[0m 801.8   \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.9372  \u001b[0m | \u001b[0m 226.4   \u001b[0m | \u001b[0m 2.863   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.826   \u001b[0m | \u001b[0m 383.5   \u001b[0m | \u001b[0m 246.2   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.8561  \u001b[0m | \u001b[0m 807.8   \u001b[0m | \u001b[0m 125.2   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 1e+03   \u001b[0m | \u001b[0m 681.2   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 188.6   \u001b[0m | \u001b[0m 997.8   \u001b[0m |\n",
      "| \u001b[95m 31      \u001b[0m | \u001b[95m 0.9763  \u001b[0m | \u001b[95m 106.0   \u001b[0m | \u001b[95m 0.001   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.7204  \u001b[0m | \u001b[0m 268.8   \u001b[0m | \u001b[0m 429.5   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.8579  \u001b[0m | \u001b[0m 488.7   \u001b[0m | \u001b[0m 102.8   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.7787  \u001b[0m | \u001b[0m 562.1   \u001b[0m | \u001b[0m 315.2   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.867   \u001b[0m | \u001b[0m 305.6   \u001b[0m | \u001b[0m 87.05   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.9098  \u001b[0m | \u001b[0m 945.1   \u001b[0m | \u001b[0m 6.873   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 609.4   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.8734  \u001b[0m | \u001b[0m 707.1   \u001b[0m | \u001b[0m 54.18   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.8024  \u001b[0m | \u001b[0m 771.3   \u001b[0m | \u001b[0m 287.1   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 153.4   \u001b[0m | \u001b[0m 2.3     \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.8297  \u001b[0m | \u001b[0m 206.4   \u001b[0m | \u001b[0m 224.0   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 221.3   \u001b[0m | \u001b[0m 708.2   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.8825  \u001b[0m | \u001b[0m 561.2   \u001b[0m | \u001b[0m 40.4    \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.8734  \u001b[0m | \u001b[0m 861.7   \u001b[0m | \u001b[0m 55.19   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.8297  \u001b[0m | \u001b[0m 918.3   \u001b[0m | \u001b[0m 224.0   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.9517  \u001b[0m | \u001b[0m 456.2   \u001b[0m | \u001b[0m 1.702   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.847   \u001b[0m | \u001b[0m 1.348   \u001b[0m | \u001b[0m 193.2   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.6867  \u001b[0m | \u001b[0m 2.612   \u001b[0m | \u001b[0m 995.0   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.9071  \u001b[0m | \u001b[0m 66.14   \u001b[0m | \u001b[0m 9.22    \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.9208  \u001b[0m | \u001b[0m 317.5   \u001b[0m | \u001b[0m 5.478   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.7268  \u001b[0m | \u001b[0m 439.2   \u001b[0m | \u001b[0m 409.5   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.8434  \u001b[0m | \u001b[0m 105.9   \u001b[0m | \u001b[0m 188.8   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.8561  \u001b[0m | \u001b[0m 395.6   \u001b[0m | \u001b[0m 123.3   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.9663  \u001b[0m | \u001b[0m 671.7   \u001b[0m | \u001b[0m 0.001   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.8306  \u001b[0m | \u001b[0m 676.9   \u001b[0m | \u001b[0m 228.2   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.8588  \u001b[0m | \u001b[0m 201.6   \u001b[0m | \u001b[0m 112.6   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.8361  \u001b[0m | \u001b[0m 510.9   \u001b[0m | \u001b[0m 206.9   \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.8734  \u001b[0m | \u001b[0m 2.23    \u001b[0m | \u001b[0m 65.28   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.8825  \u001b[0m | \u001b[0m 999.8   \u001b[0m | \u001b[0m 40.76   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.8242  \u001b[0m | \u001b[0m 999.2   \u001b[0m | \u001b[0m 248.9   \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "pbounds_new = {'C':(0.001,1000),'gamma':(0.001,1000)}\n",
    "optimizersvc = BayesianOptimization(\n",
    "    f=SVC_func_new,\n",
    "    pbounds=pbounds_new,\n",
    "    verbose=2,\n",
    "    random_state=0,\n",
    ")\n",
    "optimizersvc.maximize(n_iter=50,init_points=10)\n",
    "# weight of RF; features: 10,30,50,70; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC_func_new_sig(C,gamma):\n",
    "    clf_new1_sig = SVC(C=C,kernel='sigmoid',gamma=gamma).fit(X_train_scaled_new,y_train_new)\n",
    "    return clf_new1_sig.score(X_test_scaled_new,y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     C     |   gamma   |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5865  \u001b[0m | \u001b[0m 86.83   \u001b[0m | \u001b[0m 0.7155  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5874  \u001b[0m | \u001b[95m 94.39   \u001b[0m | \u001b[95m 0.5453  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5883  \u001b[0m | \u001b[95m 69.31   \u001b[0m | \u001b[95m 0.6462  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5656  \u001b[0m | \u001b[0m 71.26   \u001b[0m | \u001b[0m 0.8919  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.582   \u001b[0m | \u001b[0m 144.9   \u001b[0m | \u001b[0m 0.3841  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.5911  \u001b[0m | \u001b[95m 11.28   \u001b[0m | \u001b[95m 0.258   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5537  \u001b[0m | \u001b[0m 86.82   \u001b[0m | \u001b[0m 0.7467  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5893  \u001b[0m | \u001b[0m 126.4   \u001b[0m | \u001b[0m 0.6402  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5856  \u001b[0m | \u001b[0m 93.94   \u001b[0m | \u001b[0m 0.2727  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.561   \u001b[0m | \u001b[0m 86.21   \u001b[0m | \u001b[0m 0.8231  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.5865  \u001b[0m | \u001b[0m 103.6   \u001b[0m | \u001b[0m 0.5311  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.5628  \u001b[0m | \u001b[0m 34.88   \u001b[0m | \u001b[0m 0.9207  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.5601  \u001b[0m | \u001b[0m 21.42   \u001b[0m | \u001b[0m 0.8145  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.5628  \u001b[0m | \u001b[0m 82.26   \u001b[0m | \u001b[0m 0.9502  \u001b[0m |\n",
      "| \u001b[95m 15      \u001b[0m | \u001b[95m 0.602   \u001b[0m | \u001b[95m 126.3   \u001b[0m | \u001b[95m 0.169   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.5628  \u001b[0m | \u001b[0m 61.34   \u001b[0m | \u001b[0m 0.9874  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.5865  \u001b[0m | \u001b[0m 23.01   \u001b[0m | \u001b[0m 0.5579  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.5638  \u001b[0m | \u001b[0m 143.3   \u001b[0m | \u001b[0m 0.8435  \u001b[0m |\n",
      "| \u001b[95m 19      \u001b[0m | \u001b[95m 0.6111  \u001b[0m | \u001b[95m 23.26   \u001b[0m | \u001b[95m 0.1189  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.5847  \u001b[0m | \u001b[0m 94.1    \u001b[0m | \u001b[0m 0.7134  \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.5583  \u001b[0m | \u001b[0m 12.36   \u001b[0m | \u001b[0m 0.773   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.592   \u001b[0m | \u001b[0m 148.0   \u001b[0m | \u001b[0m 0.2845  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.5893  \u001b[0m | \u001b[0m 84.33   \u001b[0m | \u001b[0m 0.6466  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.5838  \u001b[0m | \u001b[0m 50.26   \u001b[0m | \u001b[0m 0.3388  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.561   \u001b[0m | \u001b[0m 83.59   \u001b[0m | \u001b[0m 0.8096  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.5856  \u001b[0m | \u001b[0m 122.3   \u001b[0m | \u001b[0m 0.6171  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.592   \u001b[0m | \u001b[0m 69.16   \u001b[0m | \u001b[0m 0.2828  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.592   \u001b[0m | \u001b[0m 82.91   \u001b[0m | \u001b[0m 0.231   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.582   \u001b[0m | \u001b[0m 93.67   \u001b[0m | \u001b[0m 0.3831  \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.5619  \u001b[0m | \u001b[0m 148.5   \u001b[0m | \u001b[0m 0.9347  \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "pbounds_new_sig = {'C':(10,150),'gamma':(0.001,1)}\n",
    "optimizersvc_sig = BayesianOptimization(\n",
    "    f=SVC_func_new_sig,\n",
    "    pbounds=pbounds_new_sig,\n",
    "    verbose=2,\n",
    "    random_state=0,\n",
    ")\n",
    "optimizersvc_sig.maximize(n_iter=25,init_points=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC_func_new_lin(C,gamma):\n",
    "    clf_new1_lin = SVC(C=int(C),kernel='linear',gamma=gamma).fit(X_train_scaled_new,y_train)\n",
    "    return clf_new1_lin.score(X_test_scaled_new,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds_new_lin = {'C':(10,150),'gamma':(0.001,1)}\n",
    "optimizersvc_lin = BayesianOptimization(\n",
    "    f=SVC_func_new_lin,\n",
    "    pbounds=pbounds_new_lin,\n",
    "    verbose=2,\n",
    "    random_state=0,\n",
    ")\n",
    "optimizersvc_lin.maximize(n_iter=10,init_points=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set: 1.000000\n",
      "accuracy on test set: 0.999089\n"
     ]
    }
   ],
   "source": [
    "forest_new = RandomForestClassifier(n_estimators=100,random_state=3,bootstrap=True, class_weight=None, criterion='gini',\n",
    " max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    " min_samples_leaf=1, min_samples_split=2,\n",
    " min_weight_fraction_leaf=0.0, n_jobs=1,\n",
    " oob_score=False, verbose=0, warm_start=False).fit(X_train_new,y_train_new)\n",
    "\n",
    "print('accuracy on training set: %f' %forest_new.score(X_train_new,y_train_new))\n",
    "print('accuracy on test set: %f' %forest_new.score(X_test_new,y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
